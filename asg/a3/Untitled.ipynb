{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download Twitter data labeled by sentiment.\n",
    "\n",
    "from StringIO import StringIO\n",
    "from zipfile import ZipFile\n",
    "from urllib import urlopen\n",
    "\n",
    "# The file is 78M, so this will take a while.\n",
    "url = urlopen('http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip')\n",
    "zipfile = ZipFile(StringIO(url.read()))\n",
    "# We'll focus on the smaller file that was manually labeled.\n",
    "# The larger file has 1.6M tweets \"pseudo-labeled\" using emoticons\n",
    "tweet_file = zipfile.open('testdata.manual.2009.06.14.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 498 tweets\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "file_reader = csv.reader(tweet_file, delimiter=',', quotechar='\"')\n",
    "tweets = []\n",
    "for row in file_reader:\n",
    "    tweets.append({'label': int(row[0]),\n",
    "                   'text': row[5]})\n",
    "print 'read %d tweets' % len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label counts= Counter({4: 182, 0: 177, 2: 139})\n"
     ]
    }
   ],
   "source": [
    "# Create label vector (y) and print its stats.\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "y = np.array([t['label'] for t in tweets])\n",
    "print 'label counts=', Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized 498 tweets. found 2264 terms.\n"
     ]
    }
   ],
   "source": [
    "# Create feature vectors (X)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(t['text'] for t in tweets)\n",
    "print 'vectorized %d tweets. found %d terms.' % (X.shape[0], X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'msgs', 1325),\n",
       " (u'whoopi', 2176),\n",
       " (u'sleep', 1804),\n",
       " (u'6pm', 67),\n",
       " (u'hate', 920),\n",
       " (u'whose', 2177),\n",
       " (u'boortz', 317),\n",
       " (u'davehitt', 557),\n",
       " (u'bike', 276),\n",
       " (u'under', 2072)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print part of the vocabulary.\n",
    "vectorizer.vocabulary_.items()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1961 1998  988 ..., 1363 1364    0]\n",
      "[u'00' u'000' u'04fo' ..., u'zomg' u'zoom' u'zydrunas']\n",
      "[u'the' u'to' u'http' ..., u'nerd' u'nerdy' u'00']\n",
      "top_terms:\n",
      "the 1961\n",
      "to 1998\n",
      "http 988\n",
      "is 1060\n",
      "and 152\n",
      "at 209\n",
      "it 1062\n",
      "for 790\n",
      "my 1337\n",
      "of 1416\n"
     ]
    }
   ],
   "source": [
    "#What are the most frequent terms?\n",
    "# Sum columns:\n",
    "col_sums = X.sum(axis=0).tolist()[0]\n",
    "#print col_sums\n",
    "top_indices = np.argsort(col_sums)[::-1]\n",
    "print top_indices\n",
    "vocab = np.array(vectorizer.get_feature_names())\n",
    "top_terms = vocab[top_indices]\n",
    "print vocab\n",
    "print top_terms\n",
    "print 'top_terms:\\n', '\\n'.join('%s %d' % (term, count) for term, count in zip(top_terms, top_indices)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "What are the most frequent terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit a LogisticRegression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training data=0.996\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy\n",
    "def accuracy(truth, predicted):\n",
    "    return (1. * len([1 for tr, pr in zip(truth, predicted) if tr == pr]) / len(truth))\n",
    "\n",
    "predicted = model.predict(X)\n",
    "print 'accuracy on training data=%.3f' % accuracy(y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10560692 -0.04011087 -0.10644479 ..., -0.11934086 -0.01161168\n",
      " -0.11164278]\n"
     ]
    }
   ],
   "source": [
    "# What are the top weighted features?\n",
    "\n",
    "# Get the learned coefficients for the Positive class.\n",
    "coef = model.coef_[0]\n",
    "print coef\n",
    "top_coef_ind = np.argsort(coef)[::-1]\n",
    "# Get the names of those features.\n",
    "top_coef_terms = vocab[top_coef_ind]\n",
    "# Get the weights of those features\n",
    "top_coef = coef[top_coef_ind]\n",
    "# Get the names of those features.\n",
    "top_coef_terms = vocab[top_coef_ind]\n",
    "# Get the weights of those features\n",
    "top_coef = coef[top_coef_ind]\n",
    "# Print the top 10.\n",
    "print 'top weighted terms for positive class:\\n', \\\n",
    "    '\\n'.join('%s %.2f' % (term, weight) for term, weight in zip(top_coef_terms, top_coef)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
